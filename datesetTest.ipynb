{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f4ab0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e0c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_CIFAR_dataset(\n",
    "    dataset,\n",
    "    file_name: str,\n",
    "    balanced: bool,\n",
    "    matrix,\n",
    "    n_clients: int,\n",
    "    n_classes: int,\n",
    "    train: bool,\n",
    "):\n",
    "    \"\"\"Partition dataset into `n_clients`.\n",
    "    Each client i has matrix[k, i] of data of class k\"\"\"\n",
    "\n",
    "    list_clients_X = [[] for i in range(n_clients)]\n",
    "    list_clients_y = [[] for i in range(n_clients)]\n",
    "    list_clients_shannon = []\n",
    "\n",
    "    if balanced and train:\n",
    "        n_samples = [500] * n_clients\n",
    "    elif balanced and not train:\n",
    "        n_samples = [100] * n_clients\n",
    "    elif not balanced and train:\n",
    "        n_samples = (\n",
    "                [100] * 2 + [250] * 10 + [500] * 10 + [750] * 5 + [1000] * 3 + [100] * 8 + [250] * 20 + [500] * 20 + [\n",
    "            750] * 15 + [1000] * 7\n",
    "        )\n",
    "    elif not balanced and not train:\n",
    "        n_samples = [20] * 2 + [50] * 10 + [100] * 10 + [150] * 5 + [200] * 3 + [20] * 8 + [50] * 20 + [100] * 20 + [\n",
    "            150] * 15 + [200] * 7\n",
    "\n",
    "\n",
    "    list_idx = []\n",
    "    for k in range(n_classes):\n",
    "\n",
    "        idx_k = np.where(np.array(dataset.targets) == k)[0]\n",
    "        list_idx += [idx_k]             # 第一维：标签值 第二维：属于该标签的条目下标\n",
    "\n",
    "    for idx_client, n_sample in enumerate(n_samples):\n",
    "        # 客户下标    客户样本数\n",
    "\n",
    "        clients_idx_i = []  # client_i 分到的数据条目下标\n",
    "        client_samples = 0\n",
    "        client_shannon = 0  # 香农指数\n",
    "\n",
    "        for k in range(n_classes):\n",
    "\n",
    "            if k < 9:\n",
    "                samples_digit = int(matrix[idx_client, k] * n_sample)\n",
    "            if k == 9:\n",
    "                samples_digit = n_sample - client_samples\n",
    "            client_samples += samples_digit\n",
    "\n",
    "            p_k = samples_digit / n_sample          # 样本比例\n",
    "            if p_k != 0:\n",
    "                client_shannon -= p_k * math.log(p_k)   # 香农指数计算\n",
    "\n",
    "            clients_idx_i = np.concatenate(\n",
    "                (clients_idx_i, np.random.choice(list_idx[k], samples_digit))   # 将标签k的数据随机选取分给client_i\n",
    "            )\n",
    "\n",
    "        # clients_idx_i 当前客户所持有数据 在数据集中的下标\n",
    "        clients_idx_i = clients_idx_i.astype(int)\n",
    "\n",
    "        list_clients_shannon.append(client_shannon)\n",
    "\n",
    "        for idx_sample in clients_idx_i:\n",
    "\n",
    "            list_clients_X[idx_client] += [dataset.data[idx_sample]]       # 客户idx_client 数据样本\n",
    "            list_clients_y[idx_client] += [dataset.targets[idx_sample]]    # 客户idx_client 数据标签\n",
    "\n",
    "        list_clients_X[idx_client] = np.array(list_clients_X[idx_client])\n",
    "\n",
    "    folder = \"./data/\"\n",
    "    with open(folder + file_name, \"wb\") as output:\n",
    "        pickle.dump((list_clients_X, list_clients_y, list_clients_shannon), output)\n",
    "        \n",
    "def create_MNIST_dirichlet(\n",
    "    dataset_name: str,\n",
    "    balanced: bool,\n",
    "    alpha: float,\n",
    "    n_clients: int,\n",
    "    n_classes: int,\n",
    "):\n",
    "\n",
    "    from numpy.random import dirichlet\n",
    "\n",
    "    # matrix = dirichlet([alpha] * n_classes, size=n_clients)\n",
    "    matrix = np.concatenate((dirichlet([alpha * 100] * n_classes, size=30), dirichlet([alpha] * n_classes, size=70)))\n",
    "\n",
    "    MNIST_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "    MNIST_test = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    file_name_train = f\"{dataset_name}_train_{n_clients}.pkl\"\n",
    "    partition_CIFAR_dataset(\n",
    "        MNIST_train,\n",
    "        file_name_train,\n",
    "        balanced,\n",
    "        matrix,\n",
    "        n_clients,\n",
    "        n_classes,\n",
    "        True,\n",
    "    )\n",
    "\n",
    "    file_name_test = f\"{dataset_name}_test_{n_clients}.pkl\"\n",
    "    partition_CIFAR_dataset(\n",
    "        MNIST_test,\n",
    "        file_name_test,\n",
    "        balanced,\n",
    "        matrix,\n",
    "        n_clients,\n",
    "        n_classes,\n",
    "        False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e176e24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
