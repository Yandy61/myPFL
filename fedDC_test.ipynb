{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2492468f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========>>> FedDC 超参数 <<<========\n",
      "  全局轮次:  600\n",
      "  本地轮次:  20\n",
      "  batch size:  50\n",
      "  汇报频率:  5\n",
      "  alpha:  0.01\n",
      "  文件名:  fedDC_MNIST_nbal_0.001_MNIST_Net_lr0.2_d1_iter600_test\n",
      "===============================\n",
      "./data/MNIST_nbal_0.001_train_100.pkl\n",
      "./data/MNIST_nbal_0.001_test_100.pkl\n",
      "MNIST_nonIID数据集加载成功！\n",
      "MNISTNet\n",
      "模型加载完成：MNIST_Net\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "import os\n",
    "import sys\n",
    "from py_func.read_db import get_dataloaders\n",
    "from py_func.create_model import load_model\n",
    "\n",
    "# 获取输入参数\n",
    "dataset = \"MNIST_nbal_0.001\"           # 数据集\n",
    "mod = \"MNIST_Net\"               # 模型\n",
    "lr = 0.2         # 学习率\n",
    "decay = 1      # 分组衰减\n",
    "n_iter = 600       # 总轮次\n",
    "alpha_coef = 0.01\n",
    "\n",
    "# 内置默认参数\n",
    "seed = 0                # 模型种子\n",
    "n_SGD = 20              # 本地轮次\n",
    "p = 1.0                 # 选取比例\n",
    "batch_size = 50         # batch_size\n",
    "meas_perf_period = 5    # 汇报频率\n",
    "\n",
    "\n",
    "\"\"\"获取超参数\"\"\"\n",
    "# 全局轮次、batch_size、acc汇报频率\n",
    "# n_iter, batch_size, meas_perf_period = get_hyperparams(dataset, n_SGD)\n",
    "\n",
    "\"\"\"用于保存数据的文件名\"\"\"\n",
    "file_name = (\n",
    "    f\"fedDC_{dataset}_{mod}_lr{lr}_d{decay}_iter{n_iter}_test\"\n",
    ")\n",
    "\n",
    "print(\"==========>>> FedDC 超参数 <<<========\")\n",
    "print(\"  全局轮次: \", n_iter)\n",
    "print(\"  本地轮次: \", n_SGD)\n",
    "print(\"  batch size: \", batch_size)\n",
    "print(\"  汇报频率: \", meas_perf_period)\n",
    "print(\"  alpha: \", alpha_coef)\n",
    "print(\"  文件名: \", file_name)\n",
    "print(\"===============================\")\n",
    "\n",
    "\n",
    "\"\"\"获取数据集\"\"\"\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "list_dls_train, list_dls_test, shannon_list = get_dataloaders(\n",
    "    dataset, batch_size)        # 数据集加载 ！\n",
    "# 按100个clients划分好了数据集\n",
    "# shannon_list : list(K,1)，元素为每个客户端所持有数据的香农多样性指数\n",
    "\n",
    "\n",
    "\"\"\"根据数据集划分确定设备个数\"\"\"\n",
    "n_sampled = int(p * len(list_dls_train))\n",
    "# print(\"  number fo sampled clients: \", n_sampled)\n",
    "\n",
    "\n",
    "\"\"\"加载初始模型\"\"\"\n",
    "model_0 = load_model(mod, seed)\n",
    "print(f\"模型加载完成：{mod}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ed8d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_para(model, flag=True):\n",
    "    '''将模型转为参数list'''\n",
    "    if not flag:\n",
    "        return model\n",
    "\n",
    "    list_params = list(model.parameters())\n",
    "    list_params = [\n",
    "        tens_param.detach() for tens_param in list_params\n",
    "    ]\n",
    "\n",
    "    return list_params\n",
    "\n",
    "def get_mdl_params(model_list, n_par=None):\n",
    "    \n",
    "    if n_par==None:\n",
    "        exp_mdl = model_list[0]\n",
    "        n_par = 0\n",
    "        for name, param in exp_mdl.named_parameters():\n",
    "            n_par += len(param.data.reshape(-1))\n",
    "    \n",
    "    param_mat = np.zeros((len(model_list), n_par)).astype('float32')\n",
    "    for i, mdl in enumerate(model_list):\n",
    "        idx = 0\n",
    "        for name, param in mdl.named_parameters():\n",
    "            temp = param.data.cpu().numpy().reshape(-1)\n",
    "            param_mat[i, idx:idx + len(temp)] = temp\n",
    "            idx += len(temp)\n",
    "    return np.copy(param_mat)\n",
    "\n",
    "def fedDC_local_learning(\n",
    "        model,\n",
    "        alpha: float,\n",
    "        optimizer,\n",
    "        train_data,\n",
    "        n_SGD: int,\n",
    "        loss_f,\n",
    "        lr,\n",
    "        grad_global_pre,\n",
    "        grad_local_pre,\n",
    "        hist_i,\n",
    "        n_par\n",
    "    ):\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)  # 移动模型到cuda\n",
    "    model_0 = deepcopy(model)\n",
    "\n",
    "    state_update_diff = torch.tensor(-grad_local_pre + grad_global_pre,  dtype=torch.float32, device=device)\n",
    "\n",
    "    global_model_param = torch.tensor(get_mdl_params([model_0],n_par)[0], dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "    for _ in range(n_SGD):\n",
    "\n",
    "        features, labels = next(iter(train_data))\n",
    "\n",
    "        features = features.to(device)  # 移动数据到cuda\n",
    "        labels = labels.to(device)\n",
    "        # 或者  labels = labels.cuda() if torch.cuda.is_available() else labels\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(features)\n",
    "\n",
    "        batch_loss = loss_f(predictions, labels)\n",
    "\n",
    "        local_parameter = None\n",
    "        for param in model.parameters():\n",
    "            if not isinstance(local_parameter, torch.Tensor):\n",
    "                local_parameter = param.reshape(-1)\n",
    "            else:\n",
    "                local_parameter = torch.cat((local_parameter, param.reshape(-1)), 0)\n",
    "\n",
    "        loss_r = alpha/2 * torch.sum((local_parameter - (global_model_param - hist_i))*(local_parameter - (global_model_param - hist_i)))\n",
    "        loss_g = torch.sum(local_parameter * state_update_diff) / (lr * n_SGD)\n",
    "        \n",
    "        batch_loss = batch_loss + loss_r + loss_g\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def set_param_to_model(model, model_param):\n",
    "    \"\"\" 根据权重进行FedAvg聚合 \"\"\"\n",
    "    dict_param = copy.deepcopy(dict(model.named_parameters()))\n",
    "    idx = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        weights = param.data\n",
    "        length = len(weights.reshape(-1))\n",
    "        dict_param[name].data.copy_(torch.tensor(model_param[idx:idx+length].reshape(weights.shape)).to(device))\n",
    "        idx += length\n",
    "    \n",
    "    model.load_state_dict(dict_param)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be7efc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========>>> 正在初始化训练\n",
      "模型放入cuda:0\n"
     ]
    }
   ],
   "source": [
    "from py_func.FedProx import *\n",
    "import copy\n",
    "\n",
    "# GPU选择\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model_0\n",
    "training_sets = list_dls_train\n",
    "testing_sets = list_dls_test\n",
    "n_iter = n_iter + 1\n",
    "metric_period=meas_perf_period\n",
    "\n",
    "print(\"========>>> 正在初始化训练\")\n",
    "\n",
    "model.to(device)  # 移动模型到cuda\n",
    "print(f\"模型放入{device}\")\n",
    "\n",
    "# 损失函数\n",
    "loss_f = loss_classifier\n",
    "\n",
    "''' ------------------------变量初始化>>>>>>>>>>>>>>>>>>>>>>>> '''\n",
    "K = len(training_sets)  # clients总数\n",
    "\n",
    "n_samples = np.array([len(db.dataset) for db in training_sets])     # array(K)，每个client拥有的样本数量\n",
    "\n",
    "weights = n_samples / np.sum(n_samples)     # array(K)，每个client样本数量所占权重\n",
    "\n",
    "loss_hist = np.zeros((n_iter + 1, K))       # array(n+1,k),记录n轮、k个设备的loss(全局模型)\n",
    "acc_hist = np.zeros((n_iter + 1, K))        # array(n+1,k),记录n轮、k个设备的acc(全局模型)\n",
    "dc_loss_hist = np.zeros((n_iter + 1, K))       # array(n+1,k),记录n轮、k个设备的loss(全局模型)\n",
    "dc_acc_hist = np.zeros((n_iter + 1, K))        # array(n+1,k),记录n轮、k个设备的acc(全局模型)\n",
    "\n",
    "server_loss_hist = []       # list(n,1),记录全局模型n轮的loss\n",
    "server_acc_hist = []        # list(n,1),记录全局模型n轮的acc\n",
    "dc_server_loss_hist = []       # list(n,1),记录全局模型n轮的loss\n",
    "dc_server_acc_hist = []        # list(n,1),记录全局模型n轮的acc\n",
    "\n",
    "# 初始化第0轮设备loss、acc\n",
    "for k, dl in enumerate(training_sets):\n",
    "    loss_hist[0, k] = float(loss_dataset(\n",
    "        model, dl, loss_f).detach())\n",
    "    acc_hist[0, k] = accuracy_dataset(model, dl)\n",
    "\n",
    "    dc_loss_hist[0,k] = loss_hist[0, k]\n",
    "    dc_acc_hist[0,k] = acc_hist[0, k]\n",
    "\n",
    "# 全局模型的loss和acc\n",
    "server_loss = np.dot(weights, loss_hist[0])     # 当前轮次全局模型的平均 loss\n",
    "server_acc = np.dot(weights, acc_hist[0])       # 当前轮次全局模型的平均 acc\n",
    "\n",
    "# 将初始loss、acc加入记录\n",
    "server_loss_hist.append(server_loss)\n",
    "server_acc_hist.append(server_acc)\n",
    "\n",
    "dc_server_loss_hist.append(server_loss)\n",
    "dc_server_acc_hist.append(server_acc)\n",
    "\n",
    "'''********************************************************************'''\n",
    "\n",
    "n_par = len(get_mdl_params([model])[0])\n",
    "\n",
    "weight_list = weights * K\n",
    "\n",
    "# # list(K) ,上一轮的梯度列表, 参数类型\n",
    "# gradients = get_gradients(model, [model] * K)\n",
    "\n",
    "# # h_i\n",
    "# parameter_drifts = copy.deepcopy(gradients)\n",
    "# 创建一个形状为(n_clnt, n_par)的数组，用于存储每个客户端的模型参数漂移值\n",
    "parameter_drifts = np.zeros((K, n_par)).astype('float32')\n",
    "\n",
    "# 获取初始模型的参数列表\n",
    "init_par_list=get_mdl_params([model], n_par)[0]\n",
    "# 创建一个形状为(n_clnt, n_par)的数组，用于存储每个客户端的模型参数，并将其初始化为初始模型的参数\n",
    "clnt_params_list  = np.ones(K).astype('float32').reshape(-1, 1) * init_par_list.reshape(1, -1)\n",
    "\n",
    "# 创建一个形状为(n_clnt+1, n_par)的数组，用于存储每个客户端和云端的状态梯度差异\n",
    "state_gadient_diffs = np.zeros((K+1, n_par)).astype('float32') #including cloud state\n",
    "# state_gadient_diffs = get_gradients(model, [model] * (K+1))\n",
    "\n",
    "# # 上一轮的总梯度，模型类型\n",
    "# grad = get_grad(model, model)\n",
    "# grad_para = get_model_para(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dce29312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'***********************************************'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一轮\n",
    "global_model_param = get_mdl_params([model], n_par)[0]\n",
    "        \n",
    "delta_g_sum = np.zeros(n_par)\n",
    "\n",
    "clients_params = []     # 当前轮次 所有客户端模型参数（占内存）\n",
    "clients_models = []     # 当前轮次 所有客户端模型\n",
    "\n",
    "# 根据轮次选择参与训练的客户 依据 聚合时的权重\n",
    "clients_list = np.arange(K)\n",
    "agre_weights = weights\n",
    "\n",
    "k = 0\n",
    "\n",
    "local_model = deepcopy(model)\n",
    "local_optimizer = optim.SGD(local_model.parameters(), lr=lr)\n",
    "\n",
    "'''********************** FedDC ******************'''\n",
    "local_update_last = state_gadient_diffs[k]\n",
    "global_update_last = state_gadient_diffs[-1]/weight_list[k]\n",
    "alpha = alpha_coef / weight_list[k]\n",
    "hist_i = torch.tensor(parameter_drifts[k], dtype=torch.float32, device=device) #h_i\n",
    "'''***********************************************'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8893ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model,\n",
    "# alpha: float,\n",
    "# optimizer,\n",
    "# train_data,\n",
    "# n_SGD: int,\n",
    "# loss_f,\n",
    "# lr,\n",
    "# grad_global_pre,\n",
    "# grad_local_pre,\n",
    "# hist_i,\n",
    "# n_par          \n",
    "\n",
    "# local_model,\n",
    "# alpha,\n",
    "# local_optimizer,\n",
    "# training_sets[k],\n",
    "# n_SGD,\n",
    "# loss_f,\n",
    "# lr,\n",
    "# global_update_last,\n",
    "# local_update_last,\n",
    "# hist_i,\n",
    "# n_par\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "local_model.to(device)  # 移动模型到cuda\n",
    "model_0 = deepcopy(local_model)\n",
    "\n",
    "state_update_diff = torch.tensor(-global_update_last + local_update_last,  dtype=torch.float32, device=device)\n",
    "\n",
    "global_model_param = torch.tensor(get_mdl_params([model_0],n_par)[0], dtype=torch.float32, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b997e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = training_sets[k]\n",
    "optimizer = local_optimizer\n",
    "\n",
    "features, labels = next(iter(train_data))\n",
    "\n",
    "features = features.to(device)  # 移动数据到cuda\n",
    "labels = labels.to(device)\n",
    "# 或者  labels = labels.cuda() if torch.cuda.is_available() else labels\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "predictions = local_model(features)\n",
    "\n",
    "batch_loss = loss_f(predictions, labels)\n",
    "\n",
    "local_parameter = None\n",
    "for param in local_model.parameters():\n",
    "    if not isinstance(local_parameter, torch.Tensor):\n",
    "        local_parameter = param.reshape(-1)\n",
    "    else:\n",
    "        local_parameter = torch.cat((local_parameter, param.reshape(-1)), 0)\n",
    "\n",
    "loss_r = alpha/2 * torch.sum((local_parameter - (global_model_param - hist_i))*(local_parameter - (global_model_param - hist_i)))\n",
    "loss_g = torch.sum(local_parameter * state_update_diff) / (lr * n_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a6f0fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<MulBackward0>)\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "a = loss_r\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
